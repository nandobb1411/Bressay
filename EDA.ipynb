{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN to extract the features of the image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN to interpret the sequential data extracted by CNNs, LSTM (Long Short-Term Memory) units are often used due to their efficiency in handling sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CTC Loss To handle the alignment between input sequences (the image features) and the target sequences (the transcribed text)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mA execução de células com 'bressay_conda_env' requer o pacote ipykernel.\n",
      "\u001b[1;31mExecute o seguinte comando para instalar \"ipykernel\" no ambiente do Python. \n",
      "\u001b[1;31mComando: \"conda install -n bressay_conda_env ipykernel --update-deps --force-reinstall\""
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import cv2 \n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Conv2D, MaxPooling2D, Reshape, Bidirectional, LSTM, Dense, Lambda, Activation, BatchNormalization, Dropout\n",
    "from keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'bressay-competition\\bressay\\sets\\test.txt', 'r') as file:\n",
    "    test_content = file.readlines()\n",
    "\n",
    "with open(r'bressay-competition\\bressay\\sets\\training.txt', 'r') as file:\n",
    "    training_content = file.readlines()\n",
    "\n",
    "with open(r'bressay-competition\\bressay\\sets\\validation.txt', 'r') as file:\n",
    "    validation_content = file.readlines()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counter_rasura = 0\n",
    "# if \"--xxx---\" in data_text:\n",
    "#     counter_rasura += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting line dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines_directory = r'bressay-competition\\bressay\\data\\lines'\n",
    "test = {}\n",
    "train = {}\n",
    "val = {}\n",
    "\n",
    "def count_unique_characters(*data_structures):\n",
    "    unique_characters = set()\n",
    "\n",
    "    for data_structure in data_structures:\n",
    "        for data in data_structure.values():\n",
    "            if 'txt' in data:\n",
    "                for text_entry in data['txt']:\n",
    "                    unique_characters.update(text_entry)\n",
    "\n",
    "    sorted_characters = sorted(unique_characters)\n",
    "    characters_string = ''.join(sorted_characters)\n",
    "    return characters_string\n",
    "\n",
    "\n",
    "def find_max_text_length(*data_structures):\n",
    "    max_length = 0\n",
    "    for data_structure in data_structures:\n",
    "        len_line = 0\n",
    "        for data in data_structure.values():\n",
    "            if 'txt' in data:\n",
    "                for line in data[\"txt\"]:\n",
    "                    if len(line) > len_line:\n",
    "                        len_line = len(line)\n",
    "                if len_line > max_length:\n",
    "                    max_length = len_line\n",
    "\n",
    "    return max_length\n",
    "\n",
    "def preprocess(img):\n",
    "    (h, w) = img.shape\n",
    "    \n",
    "    final_img = np.ones([64, 256])*255 # blank white image\n",
    "    \n",
    "    # crop\n",
    "    if w > 256:\n",
    "        img = img[:, :256]\n",
    "        \n",
    "    if h > 64:\n",
    "        img = img[:64, :]\n",
    "    \n",
    "    \n",
    "    final_img[:h, :w] = img\n",
    "    return cv2.rotate(final_img, cv2.ROTATE_90_CLOCKWISE)\n",
    "\n",
    "def collect_file_data_to_dataframe(dir_name, base_path):\n",
    "    data = []\n",
    "    full_path = os.path.join(base_path, dir_name)\n",
    "    \n",
    "    if os.path.isdir(full_path):\n",
    "        for file in os.listdir(full_path):\n",
    "            if file.endswith('.png'):\n",
    "                img_file_path = os.path.join(full_path, file)\n",
    "                txt_file_name = file.replace('.png', '.txt')\n",
    "                txt_file_path = os.path.join(full_path, txt_file_name)\n",
    "                \n",
    "                if os.path.exists(txt_file_path):\n",
    "                    with open(txt_file_path, 'r', encoding='utf-8') as txt_file:\n",
    "                        identity = txt_file.read()\n",
    "                        data.append([img_file_path, identity])\n",
    "    \n",
    "    return pd.DataFrame(data, columns=['FILENAME', 'IDENTITY'])\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dfs = []\n",
    "training_dfs = []\n",
    "val_dfs = []\n",
    "\n",
    "for dir_name in test_content:\n",
    "    dir_name = dir_name.strip()\n",
    "    test_dfs.append(collect_file_data_to_dataframe(dir_name, lines_directory))\n",
    "\n",
    "for dir_name in training_content:\n",
    "    dir_name = dir_name.strip()\n",
    "    training_dfs.append(collect_file_data_to_dataframe(dir_name, lines_directory))\n",
    "\n",
    "# for dir_name in val_content:\n",
    "#     dir_name = dir_name.strip()\n",
    "#     val_dfs.append(collect_file_data_to_dataframe(dir_name, lines_directory))\n",
    "\n",
    "# Concatenate all DataFrames in the lists\n",
    "test = pd.concat(test_dfs, ignore_index=True)\n",
    "train = pd.concat(training_dfs, ignore_index=True)\n",
    "# val_df = pd.concat(val_dfs, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 19628\n",
    "test_size = 56"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = []\n",
    "for i in range(train_size):\n",
    "    \n",
    "    relative_path = train.loc[i, 'FILENAME'].replace('\\\\\\\\', '\\\\')\n",
    "    img_dir = fr\"{relative_path}\"\n",
    "\n",
    "    image = cv2.imread(img_dir, cv2.IMREAD_GRAYSCALE)\n",
    "    \n",
    "    if image is not None:\n",
    "        image = preprocess(image)\n",
    "        image = image / 255.0  # Normalize\n",
    "        train_x.append(image)\n",
    "    else:\n",
    "        print(f\"Image not found or unable to load: {img_dir}\")\n",
    "\n",
    "train_x = np.array(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x = []\n",
    "for i in range(test_size):\n",
    "    \n",
    "    relative_path = test.loc[i, 'FILENAME'].replace('\\\\\\\\', '\\\\')\n",
    "    img_dir = fr\"{relative_path}\"\n",
    "\n",
    "    image = cv2.imread(img_dir, cv2.IMREAD_GRAYSCALE)\n",
    "    \n",
    "    if image is not None:\n",
    "        image = preprocess(image)\n",
    "        image = image / 255.0  # Normalize\n",
    "        test_x.append(image)\n",
    "    else:\n",
    "        print(f\"Image not found or unable to load: {img_dir}\")\n",
    "\n",
    "test_x = np.array(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = np.array(train_x).reshape(-1, 256, 64, 1)\n",
    "test_x = np.array(test_x).reshape(-1, 256, 64, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorizing txt (preparing for CTC loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    special_token_mappings = {\n",
    "        '--xxx--': '+',\n",
    "        '@@???@@': '&',\n",
    "        '##--xxx--##': '%',\n",
    "        '$$--xxx--$$': '`',\n",
    "        '##@@???@@##': '{',\n",
    "        '$$@@???@@$$': '}'\n",
    "    }\n",
    "    for token, unique_char in special_token_mappings.items():\n",
    "        text = text.replace(token, unique_char)\n",
    "    return text\n",
    "\n",
    "train['IDENTITY'] = train['IDENTITY'].apply(preprocess_text)\n",
    "test['IDENTITY'] = test['IDENTITY'].apply(preprocess_text)\n",
    "\n",
    "\n",
    "train['IDENTITY'] = train['IDENTITY'].apply(preprocess_text)\n",
    "test['IDENTITY'] = test['IDENTITY'].apply(preprocess_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"train\", 'w', encoding='utf-8') as file:\n",
    "#     # Iterate over each row in the dataframe\n",
    "#     for index, row in train.iterrows():\n",
    "#         # Write the formatted string to the file\n",
    "#         file.write(f\"{row['FILENAME']}\\t{row['IDENTITY']}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphabets = r' +&!\"%`\\'(),}-.{/0123456789$:;=#?ABCDEFGHIJKLMNOPQRSTUVWXYZ\\\\]`abcdefghijklmnopqrstuvwxyz|ª°´ºÀÁÈÉÍÓÚàáâãçèéêíóôõúü'\n",
    "max_str_len = 250\n",
    "num_of_characters = len(alphabets) + 1\n",
    "num_of_timestamps = 64\n",
    "\n",
    "\n",
    "def label_to_num(label):\n",
    "    label_num = []\n",
    "    for ch in label:\n",
    "        label_num.append(alphabets.find(ch))\n",
    "        \n",
    "    return np.array(label_num)\n",
    "\n",
    "def num_to_label(num):\n",
    "    ret = \"\"\n",
    "    for ch in num:\n",
    "        if ch == -1:  # CTC Blank\n",
    "            break\n",
    "        else:\n",
    "            ret+=alphabets[ch]\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'ra crítica por meio dos & metragens em exibição'\n",
    "print(name, '\\n',label_to_num(name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = np.ones([train_size, max_str_len]) * -1\n",
    "train_label_len = np.zeros([train_size, 1])\n",
    "train_input_len = np.ones([train_size, 1]) * (num_of_timestamps-2)\n",
    "train_output = np.zeros([train_size])\n",
    "\n",
    "for i in range(train_size):\n",
    "    train_label_len[i] = len(train.loc[i, 'IDENTITY'])\n",
    "    train_y[i, 0:len(train.loc[i, 'IDENTITY'])]= label_to_num(train.loc[i, 'IDENTITY']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y = np.ones([test_size, max_str_len]) * -1\n",
    "test_label_len = np.zeros([test_size, 1])\n",
    "test_input_len = np.ones([test_size, 1]) * (num_of_timestamps-2)\n",
    "test_output = np.zeros([test_size])\n",
    "\n",
    "for i in range(test_size):\n",
    "    test_label_len[i] = len(test.loc[i, 'IDENTITY'])\n",
    "    test_y[i, 0:len(test.loc[i, 'IDENTITY'])]= label_to_num(test.loc[i, 'IDENTITY'])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('True label : ',train.loc[100, 'IDENTITY'] , '\\ntrain_y : ',train_y[100],'\\ntrain_label_len : ',train_label_len[100], \n",
    "      '\\ntrain_input_len : ', train_input_len[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('True label : ',test.loc[10, 'IDENTITY'] , '\\ntrain_y : ',test_y[10],'\\ntrain_label_len : ',test_label_len[10], \n",
    "      '\\ntrain_input_len : ', test_input_len[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = Input(shape=(256, 64, 1), name='input')\n",
    "\n",
    "inner = Conv2D(32, (3, 3), padding='same', name='conv1', kernel_initializer='he_normal')(input_data)  \n",
    "inner = BatchNormalization()(inner)\n",
    "inner = Activation('relu')(inner)\n",
    "inner = MaxPooling2D(pool_size=(2, 2), name='max1')(inner)\n",
    "\n",
    "inner = Conv2D(64, (3, 3), padding='same', name='conv2', kernel_initializer='he_normal')(inner)\n",
    "inner = BatchNormalization()(inner)\n",
    "inner = Activation('relu')(inner)\n",
    "inner = MaxPooling2D(pool_size=(2, 2), name='max2')(inner)\n",
    "inner = Dropout(0.3)(inner)\n",
    "\n",
    "inner = Conv2D(128, (3, 3), padding='same', name='conv3', kernel_initializer='he_normal')(inner)\n",
    "inner = BatchNormalization()(inner)\n",
    "inner = Activation('relu')(inner)\n",
    "inner = MaxPooling2D(pool_size=(1, 2), name='max3')(inner)\n",
    "inner = Dropout(0.3)(inner)\n",
    "\n",
    "# CNN to RNN\n",
    "inner = Reshape(target_shape=((64, 1024)), name='reshape')(inner)\n",
    "inner = Dense(64, activation='relu', kernel_initializer='he_normal', name='dense1')(inner)\n",
    "\n",
    "## RNN\n",
    "inner = Bidirectional(LSTM(256, return_sequences=True), name = 'lstm1')(inner)\n",
    "inner = Bidirectional(LSTM(256, return_sequences=True), name = 'lstm2')(inner)\n",
    "\n",
    "## OUTPUT\n",
    "inner = Dense(num_of_characters, kernel_initializer='he_normal',name='dense2')(inner)\n",
    "y_pred = Activation('softmax', name='softmax')(inner)\n",
    "\n",
    "model = Model(inputs=input_data, outputs=y_pred)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ctc_lambda_func(args):\n",
    "    y_pred, labels, input_length, label_length = args\n",
    "    # the 2 is critical here since the first couple outputs of the RNN\n",
    "    # tend to be garbage\n",
    "    y_pred = y_pred[:, 2:, :]\n",
    "    return K.ctc_batch_cost(labels, y_pred, input_length, label_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = Input(name='gtruth_labels', shape=[max_str_len], dtype='float32')\n",
    "input_length = Input(name='input_length', shape=[1], dtype='int64')\n",
    "label_length = Input(name='label_length', shape=[1], dtype='int64')\n",
    "\n",
    "ctc_loss = Lambda(ctc_lambda_func, output_shape=(1,), name='ctc')([y_pred, labels, input_length, label_length])\n",
    "model_final = Model(inputs=[input_data, labels, input_length, label_length], outputs=ctc_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAINNNN!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_final.compile(loss={'ctc': lambda y_true, y_pred: y_pred}, optimizer=Adam(lr = 0.0001))\n",
    "\n",
    "model_final.fit(x=[train_x, train_y, train_input_len, train_label_len], y=train_output, \n",
    "                validation_data=([test_x, test_y, test_input_len, test_label_len], test_output),\n",
    "                epochs=60, batch_size=128)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bressay_conda_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
